# Neuroscience-guided-EEGViT-for-Auditory-Attention-Decoding

GitHub Repository:  
https://github.com/chenyibo11/Neuroscience-guided-EEGViT-for-Auditory-Attention-Decoding

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## üìñ Project Overview

This project addresses a fundamental challenge in **Auditory Attention Decoding (AAD)**: effectively disentangling attended speech from raw Electroencephalogram (EEG) signals.

Traditional AAD approaches often overlook key neuroscience priors:

- The brain‚Äôs hierarchical auditory processing mechanism  
- The temporal delay and asynchrony between acoustic input and neural responses  
- The interference from unattended competing speech encoded in EEG  

Ignoring these priors leads to limited decoding performance and reduced interpretability.

To overcome these limitations, we propose a **neuroscience-guided EEGViT framework** integrating hierarchical modeling, contrastive learning, and mutual information minimization.

---

## üß† Core Contributions

### 1Ô∏è‚É£ EEGViT

A Vision Transformer-inspired architecture designed for EEG modeling.

- Processes EEG signals as temporal patches  
- Mimics hierarchical auditory processing in the cortex  
- Learns multi-scale temporal representations  

---

### 2Ô∏è‚É£ Hierarchical Contrastive Learning (HCL)

- Aligns EEG embeddings with:
  - Attended speech embeddings  
  - Unattended speech embeddings  
- Speech features are extracted using WavLM  
- Supports multi-negative contrastive objectives  

---

### 3Ô∏è‚É£ Hierarchical Mutual Information Minimization (HMIM)

- Explicitly suppresses unattended speech information  
- Encourages disentangled neural representations  
- Improves robustness and interpretability  

---

## üìä Datasets

We validate our framework on three public AAD datasets:

- KUL Dataset  
- DTU Dataset  
- NJU Dataset  

The proposed framework consistently outperforms state-of-the-art baselines while producing neuroscientifically interpretable representations.

---

## üìÇ Current Repository Structure

The current repository includes:

---

## ‚ö†Ô∏è External / Optional Directories (Not Included in Repo)

The following directories are **not included** in the repository and must be prepared manually if needed:

### eegdata/
- Contains processed EEG `.npy` or `.pt` files  
- These are self-processed private data files  
- If access is required, please contact via email  

### stimulus/
- Stores extracted audio stimulus features  
- Automatically generated by running:


- Includes speech embeddings extracted via WavLM  

### PM/
- Used to store WavLM pre-trained model weights  
- Please manually download WavLM weights and place them here  

### vit-base-patch16-224/
- Stores original pre-trained EEGViT model weights  
- Optional, only needed if initializing from pretrained weights  

### model_path/
- Used to save trained model checkpoints  
- Will be automatically created during training if not present  

---

## üöÄ Getting Started

### 1Ô∏è‚É£ Clone Repository


---

### 2Ô∏è‚É£ Environment Setup

We recommend using a virtual environment:

All required dependencies are listed in:

Install them using:

---

### 3Ô∏è‚É£ Feature Extraction (Speech Embeddings)

To generate speech features using WavLM:

This will extract speech embeddings and save them into the `stimulus/` directory.

---

### 4Ô∏è‚É£ Train and Evaluate

Main script:

Run training:

You may modify arguments (e.g., subject ID, epochs, learning rate) within the script or via command-line flags as implemented.

---

## üîß Tech Stack

- Programming Language: Python 3.x  
- Deep Learning Framework: PyTorch  
- Speech Encoder: WavLM  
- EEG Processing: MNE-Python  
- Machine Learning Utilities: Scikit-learn  

---

## üìú License

This project is licensed under the MIT License.

---

## üì¨ Contact

If you require access to processed EEG data or have collaboration inquiries, please contact:

your_email_here

---

## üôè Acknowledgements

- We thank the providers of the KUL, DTU, and NJU datasets.
- The speech encoder is based on the WavLM model developed by Microsoft Research.
